{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QebSq_2woIeK",
        "Dm6M1sQeUFzu",
        "b4S1Z_xlUYmZ",
        "jMyy0LQJs9Nz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc370977f11a461db4771b65e6bcb083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e35b7f7bd0b747d1b5994bddfba6f6d8",
              "IPY_MODEL_905d5dd68bc14673b8877b1ae5039222",
              "IPY_MODEL_bb5be57fa98e41c78c5c53ec065fd56d"
            ],
            "layout": "IPY_MODEL_1a7f9f2a65134b0f97f9b959583232b6"
          }
        },
        "e35b7f7bd0b747d1b5994bddfba6f6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8d0f01c2a344d28260ea7dd237c2e8",
            "placeholder": "​",
            "style": "IPY_MODEL_50ab4640a69248b19c7fc483420b4f20",
            "value": "<h2>Jewelry Recommendation System</h2>"
          }
        },
        "905d5dd68bc14673b8877b1ae5039222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67069d8ad02a40dcbba01792a2482abd",
              "IPY_MODEL_60d9a7e3b0c34aec8d01187334f5560b"
            ],
            "layout": "IPY_MODEL_613b5b6dc30248aa98b4c52cd920a015"
          }
        },
        "bb5be57fa98e41c78c5c53ec065fd56d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6633e775ed3a4a8c966024d99f89e1d2",
            "msg_id": "",
            "outputs": []
          }
        },
        "1a7f9f2a65134b0f97f9b959583232b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8d0f01c2a344d28260ea7dd237c2e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ab4640a69248b19c7fc483420b4f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67069d8ad02a40dcbba01792a2482abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Upload Image",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b789436ff4594cda8ee7e076a4a32dbc",
            "style": "IPY_MODEL_0735bfdaf43e4b7daf78f522ff60d6f2",
            "tooltip": ""
          }
        },
        "60d9a7e3b0c34aec8d01187334f5560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Use Sample Image",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c75d5c49c42242a19ab575004f1caf1d",
            "style": "IPY_MODEL_23fe2a322ec144a79981b1a558aded9e",
            "tooltip": ""
          }
        },
        "613b5b6dc30248aa98b4c52cd920a015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b789436ff4594cda8ee7e076a4a32dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "200px"
          }
        },
        "0735bfdaf43e4b7daf78f522ff60d6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c75d5c49c42242a19ab575004f1caf1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "200px"
          }
        },
        "23fe2a322ec144a79981b1a558aded9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "6633e775ed3a4a8c966024d99f89e1d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# just use stored trained fies"
      ],
      "metadata": {
        "id": "QebSq_2woIeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu tqdm scikit-learn ipywidgets matplotlib\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q pandas pyarrow"
      ],
      "metadata": {
        "id": "5eYbz8-yoG0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c3b253-5466-433f-a024-0747290afc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/664.8 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2KTraceback (most recent call last):\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in \n",
            "exc_logging_wrapper\n",
            "\u001b[2K    status = run_func(*args)\n",
            "\u001b[2K             ^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in \n",
            "wrapper\n",
            "\u001b[2K    return func(self, options, args)\n",
            "\u001b[2K           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "\u001b[2K    requirement_set = resolver.resolve(\n",
            "\u001b[2K                      ^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", \n",
            "line 179, in resolve\n",
            "\u001b[2K    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 554, in \n",
            "prepare_linked_requirements_more\n",
            "\u001b[2K    self._complete_partial_requirements(\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 469, in \n",
            "_complete_partial_requirements\n",
            "\u001b[2K    for link, (filepath, _) in batch_download:\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/network/download.py\", line 185, in \n",
            "__call__\n",
            "\u001b[2K    content_file.write(chunk)\n",
            "\u001b[2KKeyboardInterrupt\n",
            "\u001b[2K\n",
            "\u001b[2KDuring handling of the above exception, another exception occurred:\n",
            "\u001b[2K\n",
            "\u001b[2KTraceback (most recent call last):\n",
            "\u001b[2K  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "\u001b[2K    sys.exit(main())\n",
            "\u001b[2K             ^^^^^^\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "\u001b[2K    return command.main(cmd_args)\n",
            "\u001b[2K           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in \n",
            "main\n",
            "\u001b[2K    return self._main(args)\n",
            "\u001b[2K           ^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in \n",
            "_main\n",
            "\u001b[2K    return run(options, args)\n",
            "\u001b[2K           ^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in \n",
            "exc_logging_wrapper\n",
            "\u001b[2K    logger.critical(\"Operation cancelled by user\")\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "\u001b[2K    self._log(CRITICAL, msg, args, **kwargs)\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 1634, in _log\n",
            "\u001b[2K    self.handle(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 1644, in handle\n",
            "\u001b[2K    self.callHandlers(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 1706, in callHandlers\n",
            "\u001b[2K    hdlr.handle(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 978, in handle\n",
            "\u001b[2K    self.emit(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/handlers.py\", line 75, in emit\n",
            "\u001b[2K    logging.FileHandler.emit(self, record)\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 1230, in emit\n",
            "\u001b[2K    StreamHandler.emit(self, record)\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
            "\u001b[2K    self.flush()\n",
            "\u001b[2K  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
            "\u001b[2K    self.stream.flush()\n",
            "\u001b[2KKeyboardInterrupt\n",
            "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/664.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\n",
            "\u001b[?25h^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 4, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n",
            "    from pip._internal.utils.misc import ensure_dir\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/misc.py\", line 39, in <module>\n",
            "    from pip._vendor.tenacity import retry, stop_after_delay, wait_fixed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/tenacity/__init__.py\", line 59, in <module>\n",
            "    from .wait import wait_chain  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/tenacity/wait.py\", line 197, in <module>\n",
            "    class wait_exponential_jitter(wait_base):\n",
            "  File \"<frozen abc>\", line 106, in __new__\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "class JewelryRecommenderServing:\n",
        "    def __init__(self,\n",
        "                 vector_dimension: int = 1280,\n",
        "                 index_path: str =\"\" ,\n",
        "                 metadata_path: str =\"\" ):\n",
        "\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "        # Load index and metadata\n",
        "        self.index_path = index_path\n",
        "        self.metadata_path = metadata_path\n",
        "        self.index = None\n",
        "        self.metadata = {}\n",
        "\n",
        "        # Load model for feature extraction\n",
        "        self.model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
        "        self.model.eval()\n",
        "        self.model = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Image transformation\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Lambda(lambda img: ImageOps.exif_transpose(img)),\n",
        "            transforms.Resize((640, 640)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Load the existing index and metadata\n",
        "        self.load_index_and_metadata()\n",
        "\n",
        "    def load_index_and_metadata(self) -> bool:\n",
        "        \"\"\"Load the pre-built FAISS index and metadata from files\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):\n",
        "                self.index = faiss.read_index(self.index_path)\n",
        "                with open(self.metadata_path, \"rb\") as f:\n",
        "                    self.metadata = pickle.load(f)\n",
        "                print(\"Index and metadata loaded successfully.\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Index file or metadata file not found at {self.index_path} or {self.metadata_path}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading index or metadata: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _extract_embedding(self, image_path: str) -> np.ndarray:\n",
        "        \"\"\"Extract embedding from an image using the pre-trained model\"\"\"\n",
        "        try:\n",
        "            with Image.open(image_path).convert('RGB') as img:\n",
        "                img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    embedding = self.model(img_tensor).squeeze().cpu().numpy()\n",
        "                return embedding\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_recommendations(self, query_image_path: str, num_recommendations: int = 5):\n",
        "        \"\"\"Get recommendations for a query image based on similarity\"\"\"\n",
        "        if self.index is None:\n",
        "            print(\"Index not loaded. Please check that the index path is correct.\")\n",
        "            return []\n",
        "\n",
        "        query_embedding = self._extract_embedding(query_image_path)\n",
        "        if query_embedding is None:\n",
        "            return []\n",
        "\n",
        "        # Perform the similarity search\n",
        "        search_k = min(num_recommendations * 3, self.index.ntotal)\n",
        "        distances, indices = self.index.search(query_embedding.reshape(1, -1), search_k)\n",
        "\n",
        "        results = []\n",
        "        seen_categories = set()\n",
        "\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            if idx != -1:\n",
        "                metadata = self.metadata[idx]\n",
        "                if metadata[\"full_path\"] != query_image_path:\n",
        "                    similarity_score = 1 / (1 + float(dist))\n",
        "                    if metadata.get(\"category\") not in seen_categories:\n",
        "                        result = {\n",
        "                            \"metadata\": metadata,\n",
        "                            \"distance\": float(dist),\n",
        "                            \"similarity_score\": similarity_score\n",
        "                        }\n",
        "                        results.append(result)\n",
        "                        seen_categories.add(metadata.get(\"category\"))\n",
        "\n",
        "        results.sort(key=lambda x: x[\"similarity_score\"], reverse=True)\n",
        "        return results[:num_recommendations]\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "def serve_recommendations(image_path, num_recommendations=5):\n",
        "    # Initialize the recommender with paths to your saved files\n",
        "    recommender = JewelryRecommenderServing(\n",
        "        index_path= \"/content/drive/MyDrive/chennai_TJ_project/model files/jewelry_index.idx\",\n",
        "        metadata_path=  \"/content/drive/MyDrive/chennai_TJ_project/model files/jewelry_metadata.pkl\"\n",
        "    )\n",
        "\n",
        "    # Get recommendations\n",
        "    recommendations = recommender.get_recommendations(image_path, num_recommendations)\n",
        "\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "3xoa_ze2nrGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path=\"/content/811Aa-KZRnL._AC_UY1100_.jpg\"\n",
        "serve_recommendations(image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1vp5QrdhonJi",
        "outputId": "58c6140f-443c-4ce7-9243-78c7526747d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 108MB/s] \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c7d7a1ab9e56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/811Aa-KZRnL._AC_UY1100_.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mserve_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-69712c52248b>\u001b[0m in \u001b[0;36mserve_recommendations\u001b[0;34m(image_path, num_recommendations)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_recommendations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Initialize the recommender with paths to your saved files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     recommender = JewelryRecommenderServing(\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mindex_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/chennai_TJ_project/model files/jewelry_index.idx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mmetadata_path\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0;34m\"/content/drive/MyDrive/chennai_TJ_project/model files/jewelry_metadata.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-69712c52248b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vector_dimension, index_path, metadata_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Load the existing index and metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_index_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_index_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-69712c52248b>\u001b[0m in \u001b[0;36mload_index_and_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36mread_index\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  10945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10946\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10947\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10949\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **load images data**\n"
      ],
      "metadata": {
        "id": "Dm6M1sQeUFzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install required packages and import dependencies\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "H4fuZPAoUS_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Block 2: Download and setup function\n",
        "def setup_dataset_from_drive():\n",
        "    \"\"\"\n",
        "    Downloads and sets up the jewelry dataset from Google Drive shared link\n",
        "    Returns the path to the dataset\n",
        "    \"\"\"\n",
        "    # Create base directory\n",
        "    base_dir = \"/content/extracted_jewellery_datar\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    # Extract file ID from the Google Drive link\n",
        "    file_id = \"1z445s15uuZUysdpyOYjIbWcV0CQrO5fs\"\n",
        "\n",
        "    # Construct the direct download URL\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Download location\n",
        "    # zip_path = os.path.join(base_dir, \"jewelry_dataset.zip\")\n",
        "    zip_path = \"/content/drive/MyDrive/chennai_TJ_project/datafiles/all images extracted.zip\"\n",
        "    print(\"Downloading dataset from Google Drive...\")\n",
        "    try:\n",
        "        # Download the file\n",
        "        # if you got zip file upload it and comment the below line to avoid downloading again👋\n",
        "        # gdown.download(url, zip_path, quiet=False)\n",
        "\n",
        "        print(\"\\nExtracting files...\")\n",
        "        # Extract the zip file\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(base_dir)\n",
        "\n",
        "        # Remove the zip file to save space\n",
        "        os.remove(zip_path)\n",
        "\n",
        "        # Verify the dataset path exists\n",
        "        dataset_path = \"/content/extracted_jewellery_datar/all images extracted\"\n",
        "        if os.path.exists(dataset_path):\n",
        "            print(f\"\\nDataset successfully downloaded and extracted to: {dataset_path}\")\n",
        "            # Count images\n",
        "            image_count = len(list(Path(dataset_path).rglob(\"*.[jJ][pP][gG]\")))\n",
        "            print(f\"Found {image_count} images in the dataset\")\n",
        "            return dataset_path\n",
        "        else:\n",
        "            print(f\"\\nError: Expected dataset path not found: {dataset_path}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError downloading or extracting dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# Block 3: Execute setup\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = setup_dataset_from_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXJXSvo3TpRX",
        "outputId": "8b8506aa-0a35-490f-f563-b215a47c3be1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from Google Drive...\n",
            "\n",
            "Extracting files...\n",
            "\n",
            "Dataset successfully downloaded and extracted to: /content/extracted_jewellery_datar/all images extracted\n",
            "Found 126985 images in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEPENDENCIES INSTALLATION**"
      ],
      "metadata": {
        "id": "b4S1Z_xlUYmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking installed packages:\n",
        "✓ faiss-cpu (version 1.10.0)\n",
        "✓ tqdm (version 4.67.1)\n",
        "✓ scikit-learn (version 1.6.1)\n",
        "✓ ipywidgets (version 7.7.1)\n",
        "✓ matplotlib (version 3.10.0)\n",
        "✓ torch (version 2.5.1+cu124)\n",
        "✓ torchvision (version 0.20.1+cu124)\n",
        "✓ pandas (version 2.2.2)\n",
        "✓ pyarrow (version 17.0.0)\n",
        "✓ Pillow (version 11.1.0)\n",
        "✓ gdown (version 5.2.0)\n",
        "✓ concurrent-log-handler (version 0.9.25)\n",
        "✓ plotly (version 5.24.1)\n",
        "✓ notebook (version 6.5.5)\n"
      ],
      "metadata": {
        "id": "KGRSpaNeUqFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install required packages\n",
        "!pip install -q faiss-cpu tqdm scikit-learn ipywidgets matplotlib\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q pandas pyarrow\n",
        "!pip install -q Pillow\n",
        "!pip install -q gdown\n",
        "!pip install -q concurrent-log-handler\n",
        "!pip install -q plotly  # for enhanced visualizations\n",
        "!pip install -q notebook  # ensure notebook compatibility\n",
        "!pip install -q ipywidgets\n",
        "!pip install -q typing-extensions  # for enhanced type hints\n",
        "\n",
        "# Block 2: Verify installations\n",
        "import pkg_resources\n",
        "import sys\n",
        "\n",
        "def verify_installations():\n",
        "    required_packages = [\n",
        "        'faiss-cpu',\n",
        "        'tqdm',\n",
        "        'scikit-learn',\n",
        "        'ipywidgets',\n",
        "        'matplotlib',\n",
        "        'torch',\n",
        "        'torchvision',\n",
        "        'pandas',\n",
        "        'pyarrow',\n",
        "        'Pillow',\n",
        "        'gdown',\n",
        "        'concurrent-log-handler',\n",
        "        'plotly',\n",
        "        'notebook'\n",
        "    ]\n",
        "\n",
        "    print(\"Checking installed packages:\")\n",
        "    for package in required_packages:\n",
        "        try:\n",
        "            version = pkg_resources.get_distribution(package).version\n",
        "            print(f\"✓ {package} (version {version})\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            print(f\"✗ {package} not found!\")\n",
        "\n",
        "# Run verification\n",
        "if __name__ == \"__main__\":\n",
        "    verify_installations()\n",
        "\n",
        "    # Print Python version\n",
        "    print(f\"\\nPython version: {sys.version}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Oo90VW9kSn_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db60e1ca-0823-4048-bf44-585d0450e025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-da9a27fa28a1>:14: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking installed packages:\n",
            "✓ faiss-cpu (version 1.10.0)\n",
            "✓ tqdm (version 4.67.1)\n",
            "✓ scikit-learn (version 1.6.1)\n",
            "✓ ipywidgets (version 7.7.1)\n",
            "✓ matplotlib (version 3.10.0)\n",
            "✓ torch (version 2.5.1+cu124)\n",
            "✓ torchvision (version 0.20.1+cu124)\n",
            "✓ pandas (version 2.2.2)\n",
            "✓ pyarrow (version 18.1.0)\n",
            "✓ Pillow (version 11.1.0)\n",
            "✓ gdown (version 5.2.0)\n",
            "✓ concurrent-log-handler (version 0.9.25)\n",
            "✓ plotly (version 5.24.1)\n",
            "✓ notebook (version 6.5.5)\n",
            "\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Import Dependencies\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image, ImageOps\n",
        "import faiss\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Suppress verbose logging\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
        "logging.getLogger('PIL').setLevel(logging.WARNING)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n"
      ],
      "metadata": {
        "id": "m3JHu39mSOeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced clustering\n"
      ],
      "metadata": {
        "id": "jMyy0LQJs9Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class ClusterMetrics:\n",
        "    n_clusters: int\n",
        "    silhouette: float\n",
        "    davies_bouldin: float\n",
        "    cluster_sizes: Dict[int, int]\n",
        "    inertia: float\n",
        "\n",
        "class EnhancedJewelryClusterer:\n",
        "    def __init__(self,\n",
        "                 min_clusters: int = 10,\n",
        "                 max_clusters: int = 50,\n",
        "                 random_state: int = 42):\n",
        "        self.min_clusters = min_clusters\n",
        "        self.max_clusters = max_clusters\n",
        "        self.random_state = random_state\n",
        "        self.best_model = None\n",
        "        self.cluster_centers_ = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def analyze_jewelry_types(self, metadata: List[Dict]) -> Dict[str, int]:\n",
        "        \"\"\"Analyze distribution of jewelry types in dataset\"\"\"\n",
        "        type_counts = {}\n",
        "        for item in metadata:\n",
        "            j_type = item.get('jewelry_type', 'unknown')\n",
        "            type_counts[j_type] = type_counts.get(j_type, 0) + 1\n",
        "        return type_counts\n",
        "\n",
        "    def adjust_clusters_by_complexity(self, n_samples: int) -> Tuple[int, int]:\n",
        "        \"\"\"Adjust cluster range based on dataset size and complexity\"\"\"\n",
        "        # Base calculation\n",
        "        suggested_min = max(10, n_samples // 1000)\n",
        "        suggested_max = min(50, n_samples // 100)\n",
        "\n",
        "        # Ensure reasonable bounds\n",
        "        final_min = max(self.min_clusters, suggested_min)\n",
        "        final_max = min(self.max_clusters, suggested_max)\n",
        "        final_min = min(final_min, final_max)  # Added line to ensure min_k <= max_k\n",
        "\n",
        "        return final_min, final_max\n",
        "\n",
        "    def evaluate_clustering(self,\n",
        "                          embeddings: np.ndarray,\n",
        "                          n_clusters: int) -> ClusterMetrics:\n",
        "        \"\"\"Evaluate clustering for a specific number of clusters\"\"\"\n",
        "        kmeans = KMeans(n_clusters=n_clusters,\n",
        "                       random_state=self.random_state,\n",
        "                       n_init='auto')\n",
        "\n",
        "        # Fit and predict\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Calculate metrics\n",
        "        sil_score = silhouette_score(embeddings, labels)\n",
        "        db_score = davies_bouldin_score(embeddings, labels)\n",
        "\n",
        "        # Get cluster sizes\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        cluster_sizes = dict(zip(unique, counts))\n",
        "\n",
        "        return ClusterMetrics(\n",
        "            n_clusters=n_clusters,\n",
        "            silhouette=sil_score,\n",
        "            davies_bouldin=db_score,\n",
        "            cluster_sizes=cluster_sizes,\n",
        "            inertia=kmeans.inertia_\n",
        "        )\n",
        "\n",
        "    def find_optimal_clusters(self,\n",
        "                            embeddings: np.ndarray,\n",
        "                            metadata: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Find optimal number of clusters using multiple metrics\"\"\"\n",
        "        print(\"Starting clustering analysis...\")\n",
        "\n",
        "        # Scale the embeddings\n",
        "        scaled_embeddings = self.scaler.fit_transform(embeddings)\n",
        "\n",
        "        # Adjust cluster range based on dataset size\n",
        "        min_k, max_k = self.adjust_clusters_by_complexity(len(embeddings))\n",
        "        print(f\"Analyzing cluster range: {min_k} to {max_k}\")\n",
        "\n",
        "        # Analyze jewelry types if metadata available\n",
        "        if metadata:\n",
        "            type_distribution = self.analyze_jewelry_types(metadata)\n",
        "            print(\"\\nJewelry Type Distribution:\")\n",
        "            for j_type, count in type_distribution.items():\n",
        "                print(f\"{j_type}: {count} items ({count/len(metadata)*100:.1f}%)\")\n",
        "\n",
        "        # Evaluate different cluster counts\n",
        "        metrics_list = []\n",
        "        for k in tqdm(range(min_k, max_k + 1, 2), desc=\"Evaluating clusters\"):\n",
        "            metrics = self.evaluate_clustering(scaled_embeddings, k)\n",
        "            metrics_list.append(metrics)\n",
        "\n",
        "        # Find best configuration using combined metric\n",
        "        best_metrics = max(metrics_list,\n",
        "                         key=lambda x: x.silhouette - x.davies_bouldin * 0.5)\n",
        "\n",
        "        # Fit final model with optimal clusters\n",
        "        final_model = KMeans(n_clusters=best_metrics.n_clusters,\n",
        "                           random_state=self.random_state,\n",
        "                           n_init='auto')\n",
        "        final_labels = final_model.fit_predict(scaled_embeddings)\n",
        "\n",
        "        # Store best model and cluster centers\n",
        "        self.best_model = final_model\n",
        "        self.cluster_centers_ = final_model.cluster_centers_\n",
        "\n",
        "        # Prepare detailed report\n",
        "        report = {\n",
        "            'optimal_clusters': best_metrics.n_clusters,\n",
        "            'silhouette_score': best_metrics.silhouette,\n",
        "            'davies_bouldin_score': best_metrics.davies_bouldin,\n",
        "            'cluster_distribution': best_metrics.cluster_sizes,\n",
        "            'cluster_labels': final_labels,\n",
        "            'scaled_embeddings': scaled_embeddings\n",
        "        }\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nClustering Analysis Results:\")\n",
        "        print(f\"Optimal number of clusters: {report['optimal_clusters']}\")\n",
        "        print(f\"Silhouette Score: {report['silhouette_score']:.3f}\")\n",
        "        print(f\"Davies-Bouldin Score: {report['davies_bouldin_score']:.3f}\")\n",
        "\n",
        "        print(\"\\nCluster Size Distribution:\")\n",
        "        for cluster, size in report['cluster_distribution'].items():\n",
        "            percentage = (size / len(embeddings)) * 100\n",
        "            print(f\"Cluster {cluster}: {size} items ({percentage:.1f}%)\")\n",
        "\n",
        "        return report\n",
        "\n",
        "    def predict(self, embeddings: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict clusters for new embeddings\"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"Model not fitted. Run find_optimal_clusters first.\")\n",
        "\n",
        "        scaled_embeddings = self.scaler.transform(embeddings)\n",
        "        return self.best_model.predict(scaled_embeddings)\n",
        "\n",
        "# Integration with JewelryRecommender\n",
        "def enhanced_auto_categorize_images(self, embeddings_array: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Enhanced auto-categorization method for JewelryRecommender class\n",
        "    \"\"\"\n",
        "    clusterer = EnhancedJewelryClusterer()\n",
        "    clustering_report = clusterer.find_optimal_clusters(\n",
        "        embeddings_array,\n",
        "        metadata=list(self.metadata.values()) if hasattr(self, 'metadata') else None\n",
        "    )\n",
        "\n",
        "    # Store clustering information in metadata\n",
        "    self.metadata['clustering_info'] = {\n",
        "        'optimal_clusters': clustering_report['optimal_clusters'],\n",
        "        'silhouette_score': clustering_report['silhouette_score'],\n",
        "        'davies_bouldin_score': clustering_report['davies_bouldin_score'],\n",
        "        'cluster_centers': clusterer.cluster_centers_.tolist()\n",
        "    }\n",
        "\n",
        "    return clustering_report['cluster_labels']"
      ],
      "metadata": {
        "id": "55022xV2bmbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main call**"
      ],
      "metadata": {
        "id": "zg-JFEXPx11i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "dc370977f11a461db4771b65e6bcb083",
            "e35b7f7bd0b747d1b5994bddfba6f6d8",
            "905d5dd68bc14673b8877b1ae5039222",
            "bb5be57fa98e41c78c5c53ec065fd56d",
            "1a7f9f2a65134b0f97f9b959583232b6",
            "6d8d0f01c2a344d28260ea7dd237c2e8",
            "50ab4640a69248b19c7fc483420b4f20",
            "67069d8ad02a40dcbba01792a2482abd",
            "60d9a7e3b0c34aec8d01187334f5560b",
            "613b5b6dc30248aa98b4c52cd920a015",
            "b789436ff4594cda8ee7e076a4a32dbc",
            "0735bfdaf43e4b7daf78f522ff60d6f2",
            "c75d5c49c42242a19ab575004f1caf1d",
            "23fe2a322ec144a79981b1a558aded9e",
            "6633e775ed3a4a8c966024d99f89e1d2"
          ]
        },
        "id": "ip00uPYRQste",
        "outputId": "cdefe762-d844-404f-8572-b1603f3582c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 40.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index and metadata loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<h2>Jewelry Recommendation System</h2>'), HBox(children=(Button(button_style='prima…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc370977f11a461db4771b65e6bcb083"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Block 2: JewelryRecommender Class Definition\n",
        "class JewelryRecommender:\n",
        "    def __init__(self,\n",
        "                dataset_path: str = \"\",\n",
        "                vector_dimension: int = 1280,\n",
        "                index_path: str = \"/content/drive/MyDrive/chennai_TJ_project/model files/jewelry_index.idx\",\n",
        "                metadata_path: str = \"/content/drive/MyDrive/chennai_TJ_project/model files/jewelry_metadata.pkl\",\n",
        "                vectors_path: str = \"/content/drive/MyDrive/chennai_TJ_project/model files/jewelry_vectors_1281.parquet\",\n",
        "                num_clusters: Optional[int] = None):  # Changed from fixed 10\n",
        "\n",
        "        logging.basicConfig(level=logging.ERROR)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        self.dataset_path = dataset_path\n",
        "        self.num_clusters = num_clusters\n",
        "        self.index_path = index_path\n",
        "        self.metadata_path = metadata_path\n",
        "        self.vectors_path = vectors_path\n",
        "\n",
        "        # Initialize FAISS index\n",
        "        self.index = faiss.IndexIVFFlat(\n",
        "            faiss.IndexFlatL2(vector_dimension),\n",
        "            vector_dimension,\n",
        "            min(100, max(10, int(vector_dimension * 0.1))),\n",
        "            faiss.METRIC_L2\n",
        "        )\n",
        "\n",
        "        # Load model\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "            self.model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
        "            self.model.eval()\n",
        "            self.model = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Image transformation\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Lambda(lambda img: ImageOps.exif_transpose(img)),\n",
        "            transforms.Resize((640, 640)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        self.metadata = {}\n",
        "\n",
        "    # Block 3: Feature Extraction Methods\n",
        "    def _extract_embedding(self, image_path: str) -> Optional[np.ndarray]:\n",
        "        try:\n",
        "            with Image.open(image_path).convert('RGB') as img:\n",
        "                img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    embedding = self.model(img_tensor).squeeze().cpu().numpy()\n",
        "                return embedding\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    # Integration with JewelryRecommender\n",
        "    def enhanced_auto_categorize_images(self, embeddings_array: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Enhanced auto-categorization method for JewelryRecommender class\n",
        "        \"\"\"\n",
        "        clusterer = EnhancedJewelryClusterer()\n",
        "        clustering_report = clusterer.find_optimal_clusters(\n",
        "            embeddings_array,\n",
        "            metadata=list(self.metadata.values()) if hasattr(self, 'metadata') else None\n",
        "        )\n",
        "\n",
        "        # Store clustering information in metadata\n",
        "        self.metadata['clustering_info'] = {\n",
        "            'optimal_clusters': clustering_report['optimal_clusters'],\n",
        "            'silhouette_score': clustering_report['silhouette_score'],\n",
        "            'davies_bouldin_score': clustering_report['davies_bouldin_score'],\n",
        "            'cluster_centers': clusterer.cluster_centers_.tolist()\n",
        "        }\n",
        "\n",
        "        return clustering_report['cluster_labels']\n",
        "    # Block 4: Index and Storage Management Methods\n",
        "    def save_vectors_to_parquet(self, embeddings_array: np.ndarray, image_paths: List[str]):\n",
        "        \"\"\"Save vectors to parquet file with columns for each dimension\"\"\"\n",
        "        # Create column names for each dimension\n",
        "        dim_cols = [f'dim_{i}' for i in range(embeddings_array.shape[1])]\n",
        "\n",
        "        # Create DataFrame with embeddings\n",
        "        df = pd.DataFrame(embeddings_array, columns=dim_cols)\n",
        "        df['image_path'] = image_paths\n",
        "\n",
        "        # Save to parquet\n",
        "        df.to_parquet(self.vectors_path, index=False)\n",
        "        print(f\"Vectors saved to {self.vectors_path}\")\n",
        "\n",
        "    def load_vectors_from_parquet(self) -> tuple[np.ndarray, List[str]]:\n",
        "        \"\"\"Load vectors from parquet file\"\"\"\n",
        "        if not os.path.exists(self.vectors_path):\n",
        "            return None, None\n",
        "\n",
        "        df = pd.read_parquet(self.vectors_path)\n",
        "        image_paths = df['image_path'].tolist()\n",
        "        dim_cols = [col for col in df.columns if col.startswith('dim_')]\n",
        "        embeddings_array = df[dim_cols].values\n",
        "\n",
        "        return embeddings_array, image_paths\n",
        "\n",
        "    def load_index_and_metadata(self) -> bool:\n",
        "        try:\n",
        "            if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):\n",
        "                self.index = faiss.read_index(self.index_path)\n",
        "                with open(self.metadata_path, \"rb\") as f:\n",
        "                    self.metadata = pickle.load(f)\n",
        "                print(\"Index and metadata loaded successfully.\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading index or metadata: {e}\")\n",
        "        return False\n",
        "\n",
        "    def _save_index_and_metadata(self):\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(self.index_path) or '.', exist_ok=True)\n",
        "            faiss.write_index(self.index, self.index_path)\n",
        "            with open(self.metadata_path, \"wb\") as f:\n",
        "                pickle.dump(self.metadata, f)\n",
        "            print(\"Index and metadata saved successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving index or metadata: {e}\")\n",
        "\n",
        "    # Block 5: Index Building Method\n",
        "    def build_index(self):\n",
        "        all_images = list(Path(self.dataset_path).rglob(\"*.[jJ][pP][gG]\"))\n",
        "        total_images = len(all_images)\n",
        "        print(f\"Found {total_images} images in the dataset\")\n",
        "\n",
        "        if total_images == 0:\n",
        "            print(\"No images found in the dataset. Exiting index building.\")\n",
        "            return\n",
        "\n",
        "        embeddings_list = []\n",
        "        metadata_list = []\n",
        "        image_paths = []\n",
        "\n",
        "        for path in tqdm(all_images):\n",
        "            embedding = self._extract_embedding(str(path))\n",
        "            if embedding is not None:\n",
        "                embeddings_list.append(embedding)\n",
        "                image_paths.append(str(path))\n",
        "                metadata = {\n",
        "                    \"full_path\": str(path),\n",
        "                    \"filename\": path.name\n",
        "                }\n",
        "                metadata_list.append(metadata)\n",
        "\n",
        "        if embeddings_list:\n",
        "            embeddings_array = np.vstack(embeddings_list)\n",
        "\n",
        "            # Save vectors to parquet\n",
        "            self.save_vectors_to_parquet(embeddings_array, image_paths)\n",
        "\n",
        "            # Auto-categorize images\n",
        "            categories = self.enhanced_auto_categorize_images(embeddings_array)\n",
        "            for meta, cat in zip(metadata_list, categories):\n",
        "                meta[\"category\"] = f\"Category_{cat}\"\n",
        "\n",
        "            print(\"Training the index...\")\n",
        "            self.index.train(embeddings_array)\n",
        "\n",
        "            print(\"Adding images to the index...\")\n",
        "            ids = np.arange(len(embeddings_list))\n",
        "            self.index.add_with_ids(embeddings_array, ids)\n",
        "            self.metadata = {i: meta for i, meta in enumerate(metadata_list)}\n",
        "\n",
        "            self._save_index_and_metadata()\n",
        "            print(f\"Successfully indexed {len(embeddings_list)} images\")\n",
        "\n",
        "    # Block 6: Recommendation Methods\n",
        "    def get_recommendations(self, query_image_path: str, num_recommendations: int = 5) -> List[Dict]:\n",
        "        query_embedding = self._extract_embedding(query_image_path)\n",
        "        if query_embedding is None:\n",
        "            return []\n",
        "\n",
        "        search_k = min(num_recommendations * 3, self.index.ntotal)\n",
        "        # print('search_k vallue:',search_k)\n",
        "        distances, indices = self.index.search(query_embedding.reshape(1, -1), search_k)\n",
        "\n",
        "        results = []\n",
        "        seen_categories = set()\n",
        "\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            if idx != -1:\n",
        "                metadata = self.metadata[idx]\n",
        "                if metadata[\"full_path\"] != query_image_path:\n",
        "                    similarity_score = 1 / (1 + float(dist))\n",
        "                    if metadata.get(\"category\") not in seen_categories:\n",
        "                        result = {\n",
        "                            \"metadata\": metadata,\n",
        "                            \"distance\": float(dist),\n",
        "                            \"similarity_score\": similarity_score\n",
        "                        }\n",
        "                        results.append(result)\n",
        "                        seen_categories.add(metadata.get(\"category\"))\n",
        "\n",
        "        results.sort(key=lambda x: x[\"similarity_score\"], reverse=True)\n",
        "        return results[:num_recommendations]\n",
        "\n",
        "    def display_recommendations(self, query_image_path: str, num_recommendations: int = 5):\n",
        "        recommendations = self.get_recommendations(query_image_path, num_recommendations)\n",
        "\n",
        "        if not recommendations:\n",
        "            print(\"No recommendations found.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(20, 5))\n",
        "\n",
        "        plt.subplot(1, num_recommendations + 1, 1)\n",
        "        query_img = Image.open(query_image_path).convert('RGB')\n",
        "        plt.imshow(query_img)\n",
        "        plt.title('Query Image', fontsize=10)\n",
        "        plt.axis('off')\n",
        "\n",
        "        for idx, result in enumerate(recommendations, 2):\n",
        "            plt.subplot(1, num_recommendations + 1, idx)\n",
        "            img_path = result['metadata']['full_path']\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            plt.imshow(img)\n",
        "\n",
        "            similarity = result['similarity_score']\n",
        "            plt.title(f\"Match {idx-1}\\nSimilarity: {similarity:.3f}\\nCategory: {result['metadata'].get('category', 'N/A')}\",\n",
        "                      fontsize=8)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nDetailed Recommendations:\")\n",
        "        for idx, result in enumerate(recommendations, 1):\n",
        "            print(f\"\\n{idx}. Category: {result['metadata'].get('category', 'N/A')}\")\n",
        "            print(f\"   Similarity Score: {result['similarity_score']:.3f}\")\n",
        "            print(f\"   File: {result['metadata']['filename']}\")\n",
        "\n",
        "# Block 7: Interface Creation\n",
        "def create_colab_interface(recommender):\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_upload_button_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            print(\"Upload an image to get recommendations\")\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            if uploaded:\n",
        "                filename = list(uploaded.keys())[0]\n",
        "                try:\n",
        "                    recommender.display_recommendations(filename)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image: {e}\")\n",
        "\n",
        "    def on_sample_button_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            dataset_images = list(Path(recommender.dataset_path).rglob(\"*.[jJ][pP][gG]\"))\n",
        "\n",
        "            if dataset_images:\n",
        "                sample_image = str(dataset_images[0])\n",
        "                print(f\"Using sample image: {sample_image}\")\n",
        "                recommender.display_recommendations(sample_image)\n",
        "            else:\n",
        "                print(\"No sample images found in the dataset.\")\n",
        "\n",
        "    upload_button = widgets.Button(\n",
        "        description='Upload Image',\n",
        "        button_style='primary',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    sample_button = widgets.Button(\n",
        "        description='Use Sample Image',\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    upload_button.on_click(on_upload_button_clicked)\n",
        "    sample_button.on_click(on_sample_button_clicked)\n",
        "\n",
        "    box = widgets.VBox([\n",
        "        widgets.HTML(\"<h2>Jewelry Recommendation System</h2>\"),\n",
        "        widgets.HBox([upload_button, sample_button]),\n",
        "        output_area\n",
        "    ])\n",
        "\n",
        "    display(box)\n",
        "\n",
        "# Block 8: Main Execution\n",
        "def main():\n",
        "    dataset_path = \"/content/extracted_jewellery_datar/all images extracted\"\n",
        "\n",
        "    # Initialize recommender\n",
        "    recommender = JewelryRecommender(dataset_path=dataset_path)\n",
        "\n",
        "    # Build or load index\n",
        "    if not recommender.load_index_and_metadata():\n",
        "        print(\"Building new index...\")\n",
        "        recommender.build_index()\n",
        "\n",
        "    # Create and display interactive interface\n",
        "    create_colab_interface(recommender)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}